# F2 Score ìµœì í™” í”„ë¡œì íŠ¸

Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ **F2 Score**ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
.
â”œâ”€â”€ f2_optimization.py       # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ f2-tuning-guide.md       # íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ì´ë“œ
â”œâ”€â”€ f2-examples.md           # ì‹¤í–‰ ì˜ˆì œ ë° ë¶„ì„ ì½”ë“œ
â””â”€â”€ README.md                # ì´ íŒŒì¼
```

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

### 1. F2 Score ì¤‘ì‹¬ ì„¤ê³„
- **Recallì— 4ë°° ê°€ì¤‘ì¹˜** ë¶€ì—¬ (False Negative ìµœì†Œí™”)
- ë¶ˆê· í˜• ë°ì´í„°ì…‹ì— ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ê³µê°„
- CVì™€ Test Set ëª¨ë‘ì—ì„œ í‰ê°€

### 2. í¬ê´„ì ì¸ íŒŒë¼ë¯¸í„° íƒìƒ‰
- `class_weight`: ì†Œìˆ˜ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •
- `max_depth`: íŠ¸ë¦¬ ê¹Šì´ ìµœì í™”
- `n_estimators`: ì•™ìƒë¸” í¬ê¸° ì¡°ì •
- ì´ 10ê°œ ì´ìƒì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰

### 3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ê° iterationë§ˆë‹¤ F2, Recall, F1, AUC ì¶œë ¥
- ìƒìœ„ ëª¨ë¸ Test Set ì¬ê²€ì¦
- ì¼ë°˜í™” ì„±ëŠ¥ ìë™ ë¶„ì„

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜
```bash
pip install pandas numpy scikit-learn
```

### ì‹¤í–‰
```bash
python f2_optimization.py
```

ë˜ëŠ” Jupyter Notebook:
```python
%run f2_optimization.py
```

## ğŸ“Š ì¶œë ¥ ì˜ˆì‹œ

### Cross-Validation ê²°ê³¼
```
Iter  | F2      | Rec     | F1      | AUC     | Acc     | Time(s) | Params
-----------------------------------------------------------------------------------
1     | 0.7234  | 0.8156  | 0.6892  | 0.8523  | 0.8312  | 12.34   | {'rf__class_weight': {0: 1, 1: 7}, ...}
2     | 0.7189  | 0.7998  | 0.6845  | 0.8467  | 0.8289  | 11.87   | {'rf__class_weight': 'balanced_subsample', ...}
```

### Leaderboard
```
TOP 30 Models by F2 Score (Cross-Validation)
================================================
rank | mean_f2 | std_f2 | mean_rec | mean_f1 | class_weight     | max_depth
-----|---------|--------|----------|---------|------------------|----------
1    | 0.7234  | 0.0234 | 0.8156   | 0.6892  | {0: 1, 1: 7}    | None
2    | 0.7189  | 0.0198 | 0.7998   | 0.6845  | balanced_subsamp | 40
```

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. íƒìƒ‰ íšŸìˆ˜ ì¡°ì •
```python
SEARCH_CONFIG = {
    "n_iter": 30,        # 10 (ë¹ ë¦„) ~ 100 (ì •ë°€)
    "cv": 5,             # 3 (ë¹ ë¦„) ~ 10 (ì •ë°€)
}
```

### 2. íŒŒë¼ë¯¸í„° ê³µê°„ ì¡°ì •
```python
"params": {
    # ë¶ˆê· í˜• ë¹„ìœ¨ì— ë”°ë¼ ì¡°ì •
    "rf__class_weight": [
        {0: 1, 1: 5},    # 1:10 ë¶ˆê· í˜•
        {0: 1, 1: 10},   # 1:50 ë¶ˆê· í˜•
        {0: 1, 1: 20},   # 1:100 ë¶ˆê· í˜•
    ],
    
    # Recall ìš°ì„  ì‹œ ê¹Šê²Œ
    "rf__max_depth": [30, 40, 50, None],
    
    # ì•ˆì •ì„± ìš°ì„  ì‹œ ë§ê²Œ
    "rf__n_estimators": [1000, 1500, 2000],
}
```

## ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ íŒ

### í•µì‹¬ íŒŒë¼ë¯¸í„° (ì˜í–¥ë ¥ ìˆœ)

1. **`class_weight`** â­â­â­â­â­
   - F2ì— ê°€ì¥ í° ì˜í–¥
   - ì†Œìˆ˜ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ 5~10ë°° ê¶Œì¥
   
2. **`max_depth`** â­â­â­â­
   - ê¹Šì„ìˆ˜ë¡ Recall í–¥ìƒ
   - `None` (ë¬´ì œí•œ) ì‹œë„ ê¶Œì¥
   
3. **`min_samples_leaf`** â­â­â­
   - 1~2ë¡œ ì„¤ì • ì‹œ ì„¸ë°€í•œ ë¶„ë¥˜
   - Recall í–¥ìƒì— íš¨ê³¼ì 

### ì „ëµë³„ ì„¤ì •

#### ê·¹ë‹¨ì  Recall ìš°ì„ 
```python
{
    "rf__class_weight": {0: 1, 1: 10},
    "rf__max_depth": None,
    "rf__min_samples_leaf": 1,
    "rf__criterion": "entropy",
}
```
â†’ F2 â‰ˆ 0.70+, Recall â‰ˆ 0.80+

#### ê· í˜• ì¡íŒ ì ‘ê·¼
```python
{
    "rf__class_weight": {0: 1, 1: 5},
    "rf__max_depth": 40,
    "rf__min_samples_leaf": 2,
    "rf__criterion": "gini",
}
```
â†’ F2 â‰ˆ 0.65+, Recall â‰ˆ 0.75+

## ğŸ“š ìƒì„¸ ë¬¸ì„œ

- **[íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ì´ë“œ](f2-tuning-guide.md)**: ê° íŒŒë¼ë¯¸í„°ì˜ íš¨ê³¼ì™€ ì¶”ì²œ ê°’
- **[ì‹¤í–‰ ì˜ˆì œ](f2-examples.md)**: ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¤ì •, ë¶„ì„ ì½”ë“œ, ë¬¸ì œ í•´ê²°

## ğŸ“ F2 Score ì´í•´í•˜ê¸°

### ê³µì‹
```
F2 = (1 + 2Â²) Ã— (Precision Ã— Recall) / (2Â² Ã— Precision + Recall)
   = 5 Ã— (Precision Ã— Recall) / (4 Ã— Precision + Recall)
```

### F1 vs F2 ë¹„êµ

| ì§€í‘œ | Precision ê°€ì¤‘ì¹˜ | Recall ê°€ì¤‘ì¹˜ | ìš©ë„ |
|------|------------------|---------------|------|
| **F1** | 1Ã— | 1Ã— | ê· í˜• ì¡íŒ í‰ê°€ |
| **F2** | 1Ã— | **4Ã—** | Recall ì¤‘ìš” ì‹œ |

### ì–¸ì œ F2ë¥¼ ì‚¬ìš©í•˜ë‚˜?

âœ… **ì‚¬ìš©í•´ì•¼ í•  ë•Œ**:
- ê³ ê° ì´íƒˆ ì˜ˆì¸¡ (ë†“ì¹˜ë©´ ë§¤ì¶œ ì†ì‹¤)
- ì˜ë£Œ ì§„ë‹¨ (ì•” ê²€ì¶œ, ì§ˆë³‘ ì˜ˆì¸¡)
- ì‚¬ê¸° íƒì§€ (False Negative ë¹„ìš© í¼)

âŒ **ì‚¬ìš©í•˜ì§€ ë§ì•„ì•¼ í•  ë•Œ**:
- ìŠ¤íŒ¸ í•„í„° (False Positive ë¹„ìš© í¼ â†’ Precision ì¤‘ìš”)
- ì¶”ì²œ ì‹œìŠ¤í…œ (Precisionì´ ë” ì¤‘ìš”)

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ê³¼ì í•© ê²€ì¦
```python
# CV-Test ì°¨ì´ í™•ì¸
if cv_f2 - test_f2 > 0.05:
    print("âš ï¸ ê³¼ì í•© ì˜ì‹¬")
    # â†’ max_depth ì¤„ì´ê¸°, min_samples_leaf ì¦ê°€
```

### 2. Precision ëª¨ë‹ˆí„°ë§
```python
# Precisionì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ë¬¸ì œ
if test_precision < 0.3:
    print("âš ï¸ False Positive ë„ˆë¬´ ë§ìŒ")
    # â†’ class_weight ê°€ì¤‘ì¹˜ ê°ì†Œ
```

### 3. í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
```python
# ë¶ˆê· í˜• ë¹„ìœ¨ì— ë”°ë¼ class_weight ì¡°ì •
imbalance_ratio = y_train.value_counts()[0] / y_train.value_counts()[1]
recommended_weight = min(imbalance_ratio / 2, 20)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### MemoryError
```python
# n_estimators ë˜ëŠ” max_depth ê°ì†Œ
"rf__n_estimators": [300, 500],  # 1500 â†’ 500
"rf__max_depth": [30],            # None â†’ 30
```

### ì‹¤í–‰ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼
```python
# n_iter, cv ê°ì†Œ
"n_iter": 20,  # 50 â†’ 20
"cv": 3,       # 5 â†’ 3
```

### CV-Test ì°¨ì´ í¼
```python
# ê·œì œ ê°•í™”
"rf__max_depth": [20, 30],                   # None ì œê±°
"rf__min_samples_leaf": [3, 5],              # 1 â†’ 3
"rf__min_impurity_decrease": [0.001, 0.01],  # 0.0 â†’ 0.001
```

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬

| ëª¨ë¸ | F2 Score | Recall | Precision | ì„¤ì • |
|------|----------|--------|-----------|------|
| Baseline | 0.45 | 0.52 | 0.48 | class_weight=None |
| F1 ìµœì í™” | 0.62 | 0.68 | 0.65 | class_weight="balanced" |
| **F2 ìµœì í™”** | **0.71** | **0.79** | **0.59** | **{0:1, 1:7}, max_depth=None** |

## ğŸ”— ì¶”ê°€ ë¶„ì„

### Confusion Matrix
```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred = pipe.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()
```

### Feature Importance
```python
importances = pipe.named_steps['rf'].feature_importances_
# ìƒìœ„ í”¼ì²˜ í™•ì¸
```

### Threshold ìµœì í™”
```python
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
f2_scores = (5 * precision * recall) / (4 * precision + recall)
best_threshold = thresholds[np.argmax(f2_scores)]
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ìˆ˜ì • ë° ë°°í¬ ê°€ëŠ¥

## ğŸ¤ ê¸°ì—¬

ê°œì„  ì‚¬í•­ì´ë‚˜ ë²„ê·¸ëŠ” ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”!

---

**Happy Tuning! ğŸš€**
