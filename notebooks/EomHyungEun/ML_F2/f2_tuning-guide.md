# F2 Score ìµœì í™”ë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ì´ë“œ

## ğŸ“Š F2 Scoreë€?

**F2 Score = (1 + Î²Â²) Ã— (Precision Ã— Recall) / (Î²Â² Ã— Precision + Recall)**
- Î² = 2ì¼ ë•Œ, Recallì— **4ë°°ì˜ ê°€ì¤‘ì¹˜** ë¶€ì—¬
- F1 ScoreëŠ” Precisionê³¼ Recallì„ ë™ë“±í•˜ê²Œ ë³´ì§€ë§Œ, F2ëŠ” **Recall ì¤‘ì‹¬**
- **False Negative(ë†“ì¹œ ì–‘ì„±) ìµœì†Œí™”**ê°€ ëª©í‘œ

### ì–¸ì œ F2 Scoreë¥¼ ì‚¬ìš©í•˜ë‚˜?
- ì˜ë£Œ ì§„ë‹¨ (ì•” ê²€ì¶œ, ì§ˆë³‘ ì˜ˆì¸¡)
- ì‚¬ê¸° íƒì§€ (ë†“ì¹˜ë©´ í° ì†ì‹¤)
- ê³ ê° ì´íƒˆ ì˜ˆì¸¡ (Revenue=1ì„ ë†“ì¹˜ë©´ ë§¤ì¶œ ì†ì‹¤)
- **ë¹„ìš© ë¶ˆê· í˜•**: FNì˜ ë¹„ìš© > FPì˜ ë¹„ìš©

---

## ğŸ¯ F2 Score í–¥ìƒì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì „ëµ

### 1. **class_weight (ê°€ì¥ ì¤‘ìš”!)**

```python
# ìš°ì„ ìˆœìœ„ ìˆœì„œ:
"rf__class_weight": [
    {0: 1, 1: 5},              # 1ìˆœìœ„: ì†Œìˆ˜ í´ë˜ìŠ¤ 5ë°° ê°€ì¤‘ì¹˜
    {0: 1, 1: 10},             # 2ìˆœìœ„: ë” ê·¹ë‹¨ì  ê°€ì¤‘ì¹˜
    "balanced_subsample",      # 3ìˆœìœ„: ë™ì  ê· í˜•
    "balanced",                # 4ìˆœìœ„: ì •ì  ê· í˜•
]
```

**íš¨ê³¼**: 
- ì†Œìˆ˜ í´ë˜ìŠ¤(Revenue=1)ë¥¼ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ë° í° í˜ë„í‹°
- Recall ê¸‰ê²©íˆ í–¥ìƒ (Precisionì€ ì•½ê°„ í¬ìƒ)

**ì¶”ì²œ ê°’**:
- ë¶ˆê· í˜• ë¹„ìœ¨ì´ 1:10 â†’ `{0: 1, 1: 5}`
- ë¶ˆê· í˜• ë¹„ìœ¨ì´ 1:50 â†’ `{0: 1, 1: 10}` ë˜ëŠ” `{0: 1, 1: 20}`
- ë¶ˆê· í˜• ë¹„ìœ¨ì´ 1:100+ â†’ `{0: 1, 1: 50}`

---

### 2. **max_depth (íŠ¸ë¦¬ ê¹Šì´)**

```python
"rf__max_depth": [30, 40, 50, None]  # ê¹Šê²Œ!
```

**íš¨ê³¼**:
- ê¹Šì€ íŠ¸ë¦¬ â†’ ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë³µì¡í•œ íŒ¨í„´ í¬ì°©
- `None` (ë¬´ì œí•œ) ì‚¬ìš© ì‹œ Recall ìµœëŒ€í™”
- ë‹¨, ê³¼ì í•© ìœ„í—˜ ì¦ê°€ â†’ CVë¡œ ê²€ì¦ í•„ìˆ˜

**ì¶”ì²œ ì „ëµ**:
- ì´ˆê¸° íƒìƒ‰: `[20, 30, 40, None]`
- F2 í–¥ìƒ ìš°ì„ : `[40, 50, 60, None]` (ê¹Šê²Œ)
- ê³¼ì í•© ë°©ì§€: `[20, 25, 30]` (ì–•ê²Œ)

---

### 3. **min_samples_leaf (ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ)**

```python
"rf__min_samples_leaf": [1, 2]  # ì‘ê²Œ!
```

**íš¨ê³¼**:
- 1 ë˜ëŠ” 2 ì„¤ì • ì‹œ â†’ ì†Œìˆ˜ í´ë˜ìŠ¤ í•˜ë‚˜ë§Œ ìˆì–´ë„ ë¦¬í”„ ë…¸ë“œ ìƒì„±
- Recall í–¥ìƒ (ì„¸ë°€í•œ ë¶„ë¥˜)
- ê³¼ì í•© ìœ„í—˜ ì¦ê°€

**ì¶”ì²œ ê°’**:
- F2 ìµœëŒ€í™”: `[1, 2]`
- ì•ˆì •ì„± ì¤‘ì‹œ: `[3, 5, 10]`

---

### 4. **min_samples_split (ë¶„í•  ìµœì†Œ ìƒ˜í”Œ)**

```python
"rf__min_samples_split": [2, 5, 10]
```

**íš¨ê³¼**:
- ì‘ì„ìˆ˜ë¡ ë” ë§ì€ ë¶„í•  â†’ Recall í–¥ìƒ
- 2ê°€ ìµœì†Œê°’

**ì¶”ì²œ ì „ëµ**:
- F2 ìš°ì„ : `[2, 5]`
- ê³¼ì í•© ë°©ì§€: `[10, 20]`

---

### 5. **n_estimators (íŠ¸ë¦¬ ê°œìˆ˜)**

```python
"rf__n_estimators": [500, 800, 1000, 1500]
```

**íš¨ê³¼**:
- ë§ì„ìˆ˜ë¡ ì•ˆì •ì  (ì•™ìƒë¸” íš¨ê³¼)
- 500ê°œ ì´ìƒ ê¶Œì¥
- ê³„ì‚° ì‹œê°„ ì¦ê°€

**ì¶”ì²œ ê°’**:
- ë¹ ë¥¸ ì‹¤í—˜: `[300, 500]`
- ìµœì¢… ëª¨ë¸: `[1000, 1500, 2000]`

---

### 6. **criterion (ë¶„í•  ê¸°ì¤€)**

```python
"rf__criterion": ["gini", "entropy"]
```

**íš¨ê³¼**:
- **entropy**: ì •ë³´ ì´ë“ ê¸°ë°˜, ë¶ˆê· í˜• ë°ì´í„°ì— ìœ ë¦¬
- **gini**: ë¶ˆìˆœë„ ê¸°ë°˜, ê³„ì‚° ë¹ ë¦„

**ì¶”ì²œ**:
- F2 ìš°ì„ : `"entropy"` ë¨¼ì € ì‹œë„
- ê· í˜•: ë‘˜ ë‹¤ í…ŒìŠ¤íŠ¸

---

### 7. **max_features (í”¼ì²˜ ìƒ˜í”Œë§)**

```python
"rf__max_features": ["sqrt", "log2", 0.5, 0.7]
```

**íš¨ê³¼**:
- ì‘ì„ìˆ˜ë¡ íŠ¸ë¦¬ ê°„ ë‹¤ì–‘ì„± ì¦ê°€ â†’ ì•™ìƒë¸” íš¨ê³¼
- ë„ˆë¬´ ì‘ìœ¼ë©´ ì •ë³´ ë¶€ì¡±

**ì¶”ì²œ**:
- ê¸°ë³¸: `"sqrt"` (âˆšn_features)
- ë§ì€ í”¼ì²˜: `"log2"` ë˜ëŠ” `0.5`
- ì ì€ í”¼ì²˜: `0.7` ë˜ëŠ” `0.9`

---

### 8. **min_impurity_decrease (ë¶ˆìˆœë„ ê°ì†Œ ìµœì†Œ)**

```python
"rf__min_impurity_decrease": [0.0, 0.001]
```

**íš¨ê³¼**:
- 0.0 â†’ ëª¨ë“  ë¶„í•  í—ˆìš© (Recallâ†‘)
- í¬ë©´ â†’ ì˜ë¯¸ ìˆëŠ” ë¶„í• ë§Œ (ê³¼ì í•© ë°©ì§€)

**ì¶”ì²œ**:
- F2 ìµœëŒ€í™”: `0.0`
- ê³¼ì í•© ë°©ì§€: `0.001` ~ `0.01`

---

## ğŸ”¥ ìµœì  ì¡°í•© ì˜ˆì‹œ

### **ê·¹ë‹¨ì  Recall ìš°ì„  (F2 ìµœëŒ€í™”)**
```python
{
    "rf__n_estimators": 1500,
    "rf__max_depth": None,
    "rf__class_weight": {0: 1, 1: 10},
    "rf__min_samples_split": 2,
    "rf__min_samples_leaf": 1,
    "rf__max_features": "sqrt",
    "rf__criterion": "entropy",
    "rf__min_impurity_decrease": 0.0,
}
```

### **ê· í˜• ì¡íŒ ì ‘ê·¼ (ê³¼ì í•© ë°©ì§€)**
```python
{
    "rf__n_estimators": 1000,
    "rf__max_depth": 40,
    "rf__class_weight": {0: 1, 1: 5},
    "rf__min_samples_split": 5,
    "rf__min_samples_leaf": 2,
    "rf__max_features": "sqrt",
    "rf__criterion": "entropy",
    "rf__min_impurity_decrease": 0.001,
}
```

---

## ğŸ“ˆ íŠœë‹ í”„ë¡œì„¸ìŠ¤

### 1ë‹¨ê³„: ë„“ì€ íƒìƒ‰
```python
"n_iter": 30,
"params": {
    "rf__class_weight": ["balanced", {0: 1, 1: 5}, {0: 1, 1: 10}],
    "rf__max_depth": [20, 30, None],
    "rf__n_estimators": [500, 1000],
}
```

### 2ë‹¨ê³„: ì„¸ë°€í•œ ì¡°ì •
ìƒìœ„ 3ê°œ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì£¼ë³€ íƒìƒ‰:
```python
# ì˜ˆ: ìµœì  class_weightê°€ {0: 1, 1: 5}ì˜€ë‹¤ë©´
"rf__class_weight": [{0: 1, 1: 4}, {0: 1, 1: 5}, {0: 1, 1: 6}, {0: 1, 1: 7}],
```

### 3ë‹¨ê³„: ì•™ìƒë¸” (ì„ íƒ)
ìƒìœ„ 5ê°œ ëª¨ë¸ì„ Voting/Stackingìœ¼ë¡œ ê²°í•©

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. **ê³¼ì í•© ê²€ì¦**
```python
# CV F2ì™€ Test F2ì˜ ì°¨ì´ë¥¼ í™•ì¸
if cv_f2 - test_f2 > 0.05:
    print("ê³¼ì í•© ì˜ì‹¬ â†’ max_depth, min_samples_leaf ì¦ê°€")
```

### 2. **í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸**
```python
print(y_train.value_counts(normalize=True))
# ì˜ˆ: 0: 0.85, 1: 0.15 â†’ 1:6 ë¶ˆê· í˜• â†’ {0: 1, 1: 4~6} ì ì ˆ
```

### 3. **Precisionì´ ë„ˆë¬´ ë‚®ìœ¼ë©´**
```python
# Precision < 0.3 â†’ ë„ˆë¬´ ë§ì€ False Positive
# í•´ê²°: class_weight ê°€ì¤‘ì¹˜ ê°ì†Œ (10 â†’ 5)
```

### 4. **ê³„ì‚° ì‹œê°„ ê´€ë¦¬**
```python
# n_estimators Ã— max_depth ì¦ê°€ ì‹œ ì‹œê°„ ê¸‰ì¦
# n_jobs=-1 í™œìš©, ë°°ì¹˜ í¬ê¸° ì¡°ì •
```

---

## ğŸš€ ë¹ ë¥¸ ì‹¤í–‰ ê°€ì´ë“œ

### 1. ì½”ë“œ ì‹¤í–‰
```bash
python f2_optimization.py
```

### 2. ê²°ê³¼ ë¶„ì„
- CV Leaderboardì—ì„œ **mean_test_f2** ìƒìœ„ ëª¨ë¸ í™•ì¸
- Test Leaderboardì—ì„œ **test_f2**ì™€ **test_rec** í™•ì¸
- CV-Test ì°¨ì´ê°€ ì‘ì€ ëª¨ë¸ ì„ íƒ (ì¼ë°˜í™” ì„±ëŠ¥)

### 3. íŒŒë¼ë¯¸í„° í•´ì„
```
Top 1 Model:
  rf__class_weight: {0: 1, 1: 7}     â† ì†Œìˆ˜ í´ë˜ìŠ¤ 7ë°° ê°€ì¤‘
  rf__max_depth: None                â† ë¬´ì œí•œ ê¹Šì´
  rf__min_samples_leaf: 1            â† ìµœì†Œ ê·œì œ
â†’ í•´ì„: ê·¹ë‹¨ì ìœ¼ë¡œ Recall ìš°ì„  ì „ëµ
```

---

## ğŸ“š ì¶”ê°€ ìµœì í™” ê¸°ë²•

### 1. **Threshold ì¡°ì •**
```python
# ê¸°ë³¸ threshold=0.5 ëŒ€ì‹  ìµœì ê°’ ì°¾ê¸°
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
f2_scores = (5 * precision * recall) / (4 * precision + recall)
best_threshold = thresholds[np.argmax(f2_scores)]

y_pred_optimized = (y_proba >= best_threshold).astype(int)
```

### 2. **SMOTE (ì˜¤ë²„ìƒ˜í”Œë§)**
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

### 3. **ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„**
- **XGBoost**: `scale_pos_weight` íŒŒë¼ë¯¸í„°
- **LightGBM**: `is_unbalance=True`
- **CatBoost**: `class_weights` ì§€ì›

---

## ğŸ“ í•µì‹¬ ìš”ì•½

| íŒŒë¼ë¯¸í„° | F2 í–¥ìƒ ë°©í–¥ | ë¦¬ìŠ¤í¬ |
|---------|------------|--------|
| `class_weight` | ì†Œìˆ˜ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ â†‘ (5~10ë°°) | Precision â†“ |
| `max_depth` | ê¹Šê²Œ (40~None) | ê³¼ì í•© |
| `min_samples_leaf` | ì‘ê²Œ (1~2) | ê³¼ì í•© |
| `min_samples_split` | ì‘ê²Œ (2~5) | ê³¼ì í•© |
| `n_estimators` | ë§ê²Œ (1000+) | ì‹œê°„ ì¦ê°€ |
| `criterion` | entropy ìš°ì„  | - |

**í•µì‹¬**: `class_weight` > `max_depth` > `min_samples_leaf` ìˆœìœ¼ë¡œ ì˜í–¥ë ¥ì´ í¼!
