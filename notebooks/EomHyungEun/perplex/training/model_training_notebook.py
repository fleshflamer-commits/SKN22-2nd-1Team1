
# ============================================================================
# Online Shoppers Intent Prediction - ëª¨ë¸ í•™ìŠµ & ë¹„êµ
# Jupyter Notebookìš©
# ============================================================================

# %% [markdown]
# # ğŸ›’ Online Shoppers Intent - ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸
# 
# ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.
# 
# **ëª©í‘œ:**
# - ì—¬ëŸ¬ ëª¨ë¸ (LR, RF, GB, XGB, LightGBM, CatBoost) í•™ìŠµ
# - ì„±ëŠ¥ ë¹„êµ (Accuracy, Precision, Recall, F1, ROC-AUC)
# - ìµœì  ëª¨ë¸ ì„ íƒ
# - Feature Importance ë¶„ì„

# %% [markdown]
# ## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, auc
)
import warnings
warnings.filterwarnings('ignore')

# ëª¨ë¸ë“¤
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# ê³ ê¸‰ ëª¨ë¸ (ì„ íƒì‚¬í•­)
try:
    import xgboost as xgb
    HAS_XGB = True
except:
    HAS_XGB = False
    print("âš ï¸ XGBoost not installed")

try:
    import lightgbm as lgb
    HAS_LGB = True
except:
    HAS_LGB = False
    print("âš ï¸ LightGBM not installed")

try:
    import catboost as cb
    HAS_CB = True
except:
    HAS_CB = False
    print("âš ï¸ CatBoost not installed")

# ì„¤ì •
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")

# %% [markdown]
# ## 2. ë°ì´í„° ë¡œë“œ

# %%
# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("../../data/processed/train.csv")
test_df = pd.read_csv("../../data/processed/test.csv")

print("=" * 80)
print("ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
print("=" * 80)
print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")
print(f"\nTrain columns: {train_df.columns.tolist()}")

# ê¸°ë³¸ ì •ë³´
print(f"\nâœ… Train set êµ¬ë§¤ìœ¨: {train_df['Revenue'].mean():.2%}")
print(f"âœ… Test set êµ¬ë§¤ìœ¨: {test_df['Revenue'].mean():.2%}")

# %% [markdown]
# ## 3. ë°ì´í„° ì „ì²˜ë¦¬

# %%
# íƒ€ê¹ƒê³¼ í”¼ì²˜ ë¶„ë¦¬
X_train = train_df.drop('Revenue', axis=1)
y_train = train_df['Revenue'].astype(int)

X_test = test_df.drop('Revenue', axis=1)
y_test = test_df['Revenue'].astype(int)

print("=" * 80)
print("ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 80)
print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# ë°ì´í„° íƒ€ì… í™•ì¸
print(f"\nğŸ“Š í”¼ì²˜ íƒ€ì…:")
print(X_train.dtypes.value_counts())

# ë²”ì£¼í˜• ë³€ìˆ˜ í™•ì¸
categorical_cols = X_train.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()

print(f"\nâœ… ë²”ì£¼í˜• ë³€ìˆ˜ ({len(categorical_cols)}ê°œ): {categorical_cols}")
print(f"âœ… ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ({len(numerical_cols)}ê°œ): {numerical_cols}")

# %% [markdown]
# ## 4. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (í•„ìš”ì‹œ)

# %%
# ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ìˆë‹¤ë©´ ì¸ì½”ë”©
if len(categorical_cols) > 0:
    print("=" * 80)
    print("ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©")
    print("=" * 80)

    label_encoders = {}

    for col in categorical_cols:
        if col in X_train.columns:
            le = LabelEncoder()

            # Train ë°ì´í„°ë¡œ fit
            X_train[col] = le.fit_transform(X_train[col].astype(str))

            # Test ë°ì´í„°ì— ì ìš© (ìƒˆë¡œìš´ ê°’ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬)
            X_test[col] = X_test[col].astype(str).apply(
                lambda x: le.transform([x])[0] if x in le.classes_ else -1
            )

            label_encoders[col] = le
            print(f"âœ… {col}: {len(le.classes_)} classes")

    print(f"\nâœ… ì´ {len(label_encoders)}ê°œ ë³€ìˆ˜ ì¸ì½”ë”© ì™„ë£Œ")
else:
    print("âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì—†ìŒ - ì¸ì½”ë”© ë¶ˆí•„ìš”")

# %% [markdown]
# ## 5. ìŠ¤ì¼€ì¼ë§ (ì˜µì…˜)

# %%
# ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ëª¨ë¸ì„ ìœ„í•´ ìŠ¤ì¼€ì¼ëœ ë²„ì „ë„ ì¤€ë¹„
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… StandardScaler ì ìš© ì™„ë£Œ")
print(f"   - Scaled train shape: {X_train_scaled.shape}")
print(f"   - Scaled test shape: {X_test_scaled.shape}")

# %% [markdown]
# ## 6. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ

# %%
# ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ì •ì˜
models = {}

# 1. Logistic Regression (ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ì‚¬ìš©)
models['Logistic Regression'] = {
    'model': LogisticRegression(
        max_iter=2000,
        random_state=42,
        class_weight='balanced',
        solver='lbfgs'
    ),
    'use_scaled': True
}

# 2. Decision Tree
models['Decision Tree'] = {
    'model': DecisionTreeClassifier(
        max_depth=10,
        min_samples_split=20,
        random_state=42,
        class_weight='balanced'
    ),
    'use_scaled': False
}

# 3. Random Forest
models['Random Forest'] = {
    'model': RandomForestClassifier(
        n_estimators=150,
        max_depth=15,
        min_samples_split=10,
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    ),
    'use_scaled': False
}

# 4. Gradient Boosting
models['Gradient Boosting'] = {
    'model': GradientBoostingClassifier(
        n_estimators=150,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        random_state=42
    ),
    'use_scaled': False
}

# 5. XGBoost (ì„ íƒì‚¬í•­)
if HAS_XGB:
    models['XGBoost'] = {
        'model': xgb.XGBClassifier(
            n_estimators=150,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
            use_label_encoder=False,
            eval_metric='logloss'
        ),
        'use_scaled': False
    }

# 6. LightGBM (ì„ íƒì‚¬í•­)
if HAS_LGB:
    models['LightGBM'] = {
        'model': lgb.LGBMClassifier(
            n_estimators=150,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            class_weight='balanced',
            verbose=-1
        ),
        'use_scaled': False
    }

# 7. CatBoost (ì„ íƒì‚¬í•­)
if HAS_CB:
    models['CatBoost'] = {
        'model': cb.CatBoostClassifier(
            iterations=150,
            learning_rate=0.1,
            depth=5,
            random_state=42,
            verbose=0,
            scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1])
        ),
        'use_scaled': False
    }

print("=" * 80)
print(f"ëª¨ë¸ ì •ì˜ ì™„ë£Œ: {len(models)}ê°œ")
print("=" * 80)
for name in models.keys():
    print(f"  âœ… {name}")

# %% [markdown]
# ## 7. ëª¨ë¸ í•™ìŠµ & í‰ê°€

# %%
import time

results = []

print("\n" + "=" * 80)
print("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("=" * 80)

for name, config in models.items():
    print(f"\n{'='*80}")
    print(f"ğŸ”¹ {name} í•™ìŠµ ì¤‘...")
    print("="*80)

    model = config['model']
    use_scaled = config['use_scaled']

    # ë°ì´í„° ì„ íƒ
    if use_scaled:
        X_tr = X_train_scaled
        X_te = X_test_scaled
    else:
        X_tr = X_train
        X_te = X_test

    # í•™ìŠµ ì‹œê°„ ì¸¡ì •
    start_time = time.time()

    try:
        # í•™ìŠµ
        model.fit(X_tr, y_train)

        # ì˜ˆì¸¡
        y_pred_train = model.predict(X_tr)
        y_pred_test = model.predict(X_te)
        y_proba_train = model.predict_proba(X_tr)[:, 1]
        y_proba_test = model.predict_proba(X_te)[:, 1]

        # í•™ìŠµ ì‹œê°„
        elapsed_time = time.time() - start_time

        # Train ì„±ëŠ¥
        train_acc = accuracy_score(y_train, y_pred_train)
        train_prec = precision_score(y_train, y_pred_train, zero_division=0)
        train_rec = recall_score(y_train, y_pred_train, zero_division=0)
        train_f1 = f1_score(y_train, y_pred_train, zero_division=0)
        train_auc = roc_auc_score(y_train, y_proba_train)

        # Test ì„±ëŠ¥
        test_acc = accuracy_score(y_test, y_pred_test)
        test_prec = precision_score(y_test, y_pred_test, zero_division=0)
        test_rec = recall_score(y_test, y_pred_test, zero_division=0)
        test_f1 = f1_score(y_test, y_pred_test, zero_division=0)
        test_auc = roc_auc_score(y_test, y_proba_test)

        # ê²°ê³¼ ì €ì¥
        results.append({
            'Model': name,
            'Train_Acc': train_acc,
            'Test_Acc': test_acc,
            'Train_Prec': train_prec,
            'Test_Prec': test_prec,
            'Train_Rec': train_rec,
            'Test_Rec': test_rec,
            'Train_F1': train_f1,
            'Test_F1': test_f1,
            'Train_AUC': train_auc,
            'Test_AUC': test_auc,
            'Time(s)': elapsed_time
        })

        # ëª¨ë¸ ì €ì¥ (ë”•ì…”ë„ˆë¦¬ì—)
        config['trained_model'] = model
        config['y_pred_test'] = y_pred_test
        config['y_proba_test'] = y_proba_test

        # ì¶œë ¥
        print(f"\nğŸ“Š Train ì„±ëŠ¥:")
        print(f"   Accuracy:  {train_acc:.4f}")
        print(f"   Precision: {train_prec:.4f}")
        print(f"   Recall:    {train_rec:.4f}")
        print(f"   F1-Score:  {train_f1:.4f}")
        print(f"   ROC-AUC:   {train_auc:.4f}")

        print(f"\nğŸ“Š Test ì„±ëŠ¥:")
        print(f"   Accuracy:  {test_acc:.4f}")
        print(f"   Precision: {test_prec:.4f}")
        print(f"   Recall:    {test_rec:.4f}")
        print(f"   F1-Score:  {test_f1:.4f}")
        print(f"   ROC-AUC:   {test_auc:.4f}")

        print(f"\nâ±ï¸  í•™ìŠµ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print("âœ… ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        continue

print("\n" + "=" * 80)
print("âœ¨ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("=" * 80)

# %% [markdown]
# ## 8. ê²°ê³¼ ë¹„êµ

# %%
# ê²°ê³¼ DataFrame ìƒì„±
results_df = pd.DataFrame(results)

# í¬ë§·íŒ…
for col in results_df.columns:
    if col not in ['Model', 'Time(s)']:
        results_df[col] = results_df[col].round(4)
    elif col == 'Time(s)':
        results_df[col] = results_df[col].round(2)

print("=" * 80)
print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ")
print("=" * 80)
print(results_df.to_string(index=False))

# CSV ì €ì¥
results_df.to_csv('model_comparison_results.csv', index=False, encoding='utf-8-sig')
print("\nâœ… ê²°ê³¼ ì €ì¥: model_comparison_results.csv")

# %% [markdown]
# ## 9. ì‹œê°í™” - ì„±ëŠ¥ ë¹„êµ

# %%
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Test Accuracy
ax1 = axes[0, 0]
results_df_sorted = results_df.sort_values('Test_Acc', ascending=True)
ax1.barh(results_df_sorted['Model'], results_df_sorted['Test_Acc'], color='steelblue')
ax1.set_xlabel('Accuracy', fontsize=12)
ax1.set_title('Test Accuracy ë¹„êµ', fontsize=14, fontweight='bold')
ax1.set_xlim([0.8, 1.0])
for i, v in enumerate(results_df_sorted['Test_Acc']):
    ax1.text(v + 0.005, i, f'{v:.4f}', va='center', fontsize=10)

# 2. Test F1-Score
ax2 = axes[0, 1]
results_df_sorted = results_df.sort_values('Test_F1', ascending=True)
ax2.barh(results_df_sorted['Model'], results_df_sorted['Test_F1'], color='coral')
ax2.set_xlabel('F1-Score', fontsize=12)
ax2.set_title('Test F1-Score ë¹„êµ', fontsize=14, fontweight='bold')
ax2.set_xlim([0.4, 0.8])
for i, v in enumerate(results_df_sorted['Test_F1']):
    ax2.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=10)

# 3. Test ROC-AUC
ax3 = axes[1, 0]
results_df_sorted = results_df.sort_values('Test_AUC', ascending=True)
ax3.barh(results_df_sorted['Model'], results_df_sorted['Test_AUC'], color='mediumseagreen')
ax3.set_xlabel('ROC-AUC', fontsize=12)
ax3.set_title('Test ROC-AUC ë¹„êµ', fontsize=14, fontweight='bold')
ax3.set_xlim([0.85, 1.0])
for i, v in enumerate(results_df_sorted['Test_AUC']):
    ax3.text(v + 0.005, i, f'{v:.4f}', va='center', fontsize=10)

# 4. í•™ìŠµ ì‹œê°„
ax4 = axes[1, 1]
results_df_sorted = results_df.sort_values('Time(s)', ascending=True)
ax4.barh(results_df_sorted['Model'], results_df_sorted['Time(s)'], color='mediumpurple')
ax4.set_xlabel('Time (seconds)', fontsize=12)
ax4.set_title('í•™ìŠµ ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
for i, v in enumerate(results_df_sorted['Time(s)']):
    ax4.text(v + 0.1, i, f'{v:.2f}s', va='center', fontsize=10)

plt.tight_layout()
plt.savefig('model_comparison_charts.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… ì°¨íŠ¸ ì €ì¥: model_comparison_charts.png")

# %% [markdown]
# ## 10. ì •ë°€ë„-ì¬í˜„ìœ¨-F1 ë¹„êµ

# %%
fig, ax = plt.subplots(figsize=(14, 8))

x = np.arange(len(results_df))
width = 0.25

bars1 = ax.bar(x - width, results_df['Test_Prec'], width, label='Precision', color='skyblue')
bars2 = ax.bar(x, results_df['Test_Rec'], width, label='Recall', color='lightcoral')
bars3 = ax.bar(x + width, results_df['Test_F1'], width, label='F1-Score', color='lightgreen')

ax.set_xlabel('Models', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Test Set - Precision, Recall, F1-Score ë¹„êµ', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax.legend(fontsize=11)
ax.set_ylim([0, 1])
ax.grid(axis='y', alpha=0.3)

# ê°’ í‘œì‹œ
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig('precision_recall_f1_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… ì°¨íŠ¸ ì €ì¥: precision_recall_f1_comparison.png")

# %% [markdown]
# ## 11. ROC Curve ë¹„êµ

# %%
fig, ax = plt.subplots(figsize=(12, 10))

colors = plt.cm.tab10(np.linspace(0, 1, len(models)))

for i, (name, config) in enumerate(models.items()):
    if 'y_proba_test' in config:
        fpr, tpr, _ = roc_curve(y_test, config['y_proba_test'])
        roc_auc_val = auc(fpr, tpr)
        ax.plot(fpr, tpr, color=colors[i], lw=2, 
                label=f'{name} (AUC = {roc_auc_val:.4f})')

ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5000)')
ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.set_xlabel('False Positive Rate', fontsize=12)
ax.set_ylabel('True Positive Rate', fontsize=12)
ax.set_title('ROC Curve ë¹„êµ', fontsize=14, fontweight='bold')
ax.legend(loc='lower right', fontsize=10)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… ì°¨íŠ¸ ì €ì¥: roc_curves_comparison.png")

# %% [markdown]
# ## 12. Confusion Matrix (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)

# %%
# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸° (Test AUC ê¸°ì¤€)
best_model_name = results_df.loc[results_df['Test_AUC'].idxmax(), 'Model']
best_model_config = models[best_model_name]

print("=" * 80)
print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (Test ROC-AUC ê¸°ì¤€): {best_model_name}")
print("=" * 80)

# Confusion Matrix
cm = confusion_matrix(y_test, best_model_config['y_pred_test'])

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, 
            xticklabels=['No Purchase', 'Purchase'],
            yticklabels=['No Purchase', 'Purchase'],
            ax=ax)
ax.set_xlabel('Predicted', fontsize=12)
ax.set_ylabel('Actual', fontsize=12)
ax.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig(f'confusion_matrix_{best_model_name.replace(" ", "_")}.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ… Confusion Matrix ì €ì¥: confusion_matrix_{best_model_name.replace(' ', '_')}.png")

# Classification Report
print(f"\nğŸ“Š {best_model_name} - Classification Report:")
print(classification_report(y_test, best_model_config['y_pred_test'], 
                          target_names=['No Purchase', 'Purchase']))

# %% [markdown]
# ## 13. Feature Importance (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸)

# %%
# Feature Importance ì¶”ì¶œ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤
tree_based_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM', 'CatBoost', 'Decision Tree']

for name in tree_based_models:
    if name in models and 'trained_model' in models[name]:
        model = models[name]['trained_model']

        # Feature Importance ì¶”ì¶œ
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            feature_names = X_train.columns

            # DataFrame ìƒì„±
            fi_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values('Importance', ascending=False)

            # Top 15 í”¼ì²˜
            top_n = 15
            fi_top = fi_df.head(top_n)

            # ì‹œê°í™”
            fig, ax = plt.subplots(figsize=(10, 8))
            ax.barh(fi_top['Feature'], fi_top['Importance'], color='teal')
            ax.set_xlabel('Importance', fontsize=12)
            ax.set_title(f'{name} - Feature Importance (Top {top_n})', 
                        fontsize=14, fontweight='bold')
            ax.invert_yaxis()

            for i, v in enumerate(fi_top['Importance']):
                ax.text(v + 0.001, i, f'{v:.4f}', va='center', fontsize=9)

            plt.tight_layout()
            plt.savefig(f'feature_importance_{name.replace(" ", "_")}.png', dpi=300, bbox_inches='tight')
            plt.show()

            print(f"âœ… Feature Importance ì°¨íŠ¸ ì €ì¥: feature_importance_{name.replace(' ', '_')}.png")

            # CSV ì €ì¥
            fi_df.to_csv(f'feature_importance_{name.replace(" ", "_")}.csv', 
                        index=False, encoding='utf-8-sig')
            print(f"âœ… Feature Importance ë°ì´í„° ì €ì¥: feature_importance_{name.replace(' ', '_')}.csv\n")

# %% [markdown]
# ## 14. ìµœì¢… ê²°ê³¼ ìš”ì•½

# %%
print("\n" + "=" * 80)
print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("=" * 80)

# ê° ë©”íŠ¸ë¦­ë³„ ìµœê³  ëª¨ë¸
best_by_metric = {
    'Test Accuracy': results_df.loc[results_df['Test_Acc'].idxmax()],
    'Test Precision': results_df.loc[results_df['Test_Prec'].idxmax()],
    'Test Recall': results_df.loc[results_df['Test_Rec'].idxmax()],
    'Test F1-Score': results_df.loc[results_df['Test_F1'].idxmax()],
    'Test ROC-AUC': results_df.loc[results_df['Test_AUC'].idxmax()],
    'Fastest Training': results_df.loc[results_df['Time(s)'].idxmin()]
}

for metric, row in best_by_metric.items():
    print(f"\nğŸ† {metric} ìµœê³ :")
    print(f"   ëª¨ë¸: {row['Model']}")
    if 'Time' in metric:
        print(f"   ê°’: {row['Time(s)']:.2f}ì´ˆ")
    else:
        metric_col = metric.replace(' ', '_')
        print(f"   ê°’: {row[metric_col]:.4f}")

# ì¢…í•© í‰ê°€
print("\n" + "=" * 80)
print("ğŸ’¡ ì¶”ì²œ ëª¨ë¸")
print("=" * 80)

best_overall = results_df.loc[results_df['Test_AUC'].idxmax()]
print(f"\nâœ¨ ì¢…í•© ìµœê³  ì„±ëŠ¥ (ROC-AUC ê¸°ì¤€): {best_overall['Model']}")
print(f"   - Test Accuracy:  {best_overall['Test_Acc']:.4f}")
print(f"   - Test Precision: {best_overall['Test_Prec']:.4f}")
print(f"   - Test Recall:    {best_overall['Test_Rec']:.4f}")
print(f"   - Test F1-Score:  {best_overall['Test_F1']:.4f}")
print(f"   - Test ROC-AUC:   {best_overall['Test_AUC']:.4f}")
print(f"   - í•™ìŠµ ì‹œê°„:      {best_overall['Time(s)']:.2f}ì´ˆ")

# %% [markdown]
# ## 15. ëª¨ë¸ ì €ì¥ (ì„ íƒì‚¬í•­)

# %%
import pickle

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
best_model_obj = models[best_overall['Model']]['trained_model']

with open(f'best_model_{best_overall["Model"].replace(" ", "_")}.pkl', 'wb') as f:
    pickle.dump(best_model_obj, f)

print(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: best_model_{best_overall['Model'].replace(' ', '_')}.pkl")

# ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥ (í•„ìš”ì‹œ)
if models[best_overall['Model']]['use_scaled']:
    with open('scaler_for_best_model.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: scaler_for_best_model.pkl")

# %% [markdown]
# ## 16. ë§ˆë¬´ë¦¬

# %%
print("\n" + "=" * 80)
print("âœ¨ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
print("=" * 80)
print("\nìƒì„±ëœ íŒŒì¼:")
print("  ğŸ“„ model_comparison_results.csv")
print("  ğŸ“Š model_comparison_charts.png")
print("  ğŸ“Š precision_recall_f1_comparison.png")
print("  ğŸ“Š roc_curves_comparison.png")
print(f"  ğŸ“Š confusion_matrix_{best_overall['Model'].replace(' ', '_')}.png")
print(f"  ğŸ’¾ best_model_{best_overall['Model'].replace(' ', '_')}.pkl")
print("  ğŸ“Š feature_importance_*.png (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸)")
print("  ğŸ“„ feature_importance_*.csv (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸)")
print("\në‹¤ìŒ ë‹¨ê³„:")
print("  1. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
print("  2. êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸")
print("  3. ì•™ìƒë¸” ê¸°ë²• ì ìš© ê³ ë ¤")
print("  4. Streamlit ì•±ì— í†µí•©")
