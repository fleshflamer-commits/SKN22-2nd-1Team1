
# ============================================================================
# ê°„ë‹¨ ë²„ì „ - ëª¨ë¸ í•™ìŠµ & ë¹„êµ
# ì£¼í”¼í„° ë…¸íŠ¸ë¶ìš© (í•µì‹¬ë§Œ)
# ============================================================================

# %% 1. ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# ì„ íƒì‚¬í•­
try:
    import xgboost as xgb
    HAS_XGB = True
except:
    HAS_XGB = False

try:
    import lightgbm as lgb
    HAS_LGB = True
except:
    HAS_LGB = False

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")

# %% 2. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("../../data/processed/train.csv")
test_df = pd.read_csv("../../data/processed/test.csv")

print(f"Train: {train_df.shape}, Test: {test_df.shape}")
print(f"êµ¬ë§¤ìœ¨ - Train: {train_df['Revenue'].mean():.2%}, Test: {test_df['Revenue'].mean():.2%}")

# %% 3. ë°ì´í„° ì¤€ë¹„
X_train = train_df.drop('Revenue', axis=1)
y_train = train_df['Revenue'].astype(int)
X_test = test_df.drop('Revenue', axis=1)
y_test = test_df['Revenue'].astype(int)

# ë²”ì£¼í˜• ì¸ì½”ë”© (í•„ìš”ì‹œ)
categorical_cols = X_train.select_dtypes(include=['object', 'category', 'bool']).columns
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col].astype(str))
    X_test[col] = X_test[col].astype(str).apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

# ìŠ¤ì¼€ì¼ë§ (LRìš©)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

# %% 4. ëª¨ë¸ ì •ì˜
models = {}

models['LR'] = {'model': LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced'), 'scaled': True}
models['RF'] = {'model': RandomForestClassifier(n_estimators=150, max_depth=15, random_state=42, class_weight='balanced', n_jobs=-1), 'scaled': False}
models['GB'] = {'model': GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42), 'scaled': False}

if HAS_XGB:
    models['XGB'] = {'model': xgb.XGBClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42, eval_metric='logloss'), 'scaled': False}

if HAS_LGB:
    models['LGB'] = {'model': lgb.LGBMClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42, class_weight='balanced', verbose=-1), 'scaled': False}

print(f"âœ… {len(models)}ê°œ ëª¨ë¸ ì •ì˜")

# %% 5. í•™ìŠµ & í‰ê°€
results = []

for name, config in models.items():
    print(f"\n{'='*60}")
    print(f"ğŸ”¹ {name} í•™ìŠµ ì¤‘...")

    model = config['model']
    X_tr = X_train_scaled if config['scaled'] else X_train
    X_te = X_test_scaled if config['scaled'] else X_test

    # í•™ìŠµ
    model.fit(X_tr, y_train)

    # ì˜ˆì¸¡
    y_pred = model.predict(X_te)
    y_proba = model.predict_proba(X_te)[:, 1]

    # í‰ê°€
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_proba)

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1,
        'ROC-AUC': auc
    })

    print(f"Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}")

    # ì €ì¥
    config['trained'] = model
    config['y_proba'] = y_proba

print(f"\n{'='*60}")
print("âœ… í•™ìŠµ ì™„ë£Œ!")

# %% 6. ê²°ê³¼ ë¹„êµ
results_df = pd.DataFrame(results)
print("\n" + "="*60)
print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
print("="*60)
print(results_df.to_string(index=False))

# ìµœê³  ëª¨ë¸
best = results_df.loc[results_df['ROC-AUC'].idxmax()]
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best['Model']} (AUC: {best['ROC-AUC']:.4f})")

# CSV ì €ì¥
results_df.to_csv('model_results.csv', index=False)
print("\nâœ… ê²°ê³¼ ì €ì¥: model_results.csv")

# %% 7. ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Accuracy
axes[0].barh(results_df['Model'], results_df['Accuracy'], color='steelblue')
axes[0].set_xlabel('Accuracy')
axes[0].set_title('Test Accuracy')
for i, v in enumerate(results_df['Accuracy']):
    axes[0].text(v + 0.01, i, f'{v:.4f}', va='center')

# F1-Score
axes[1].barh(results_df['Model'], results_df['F1-Score'], color='coral')
axes[1].set_xlabel('F1-Score')
axes[1].set_title('Test F1-Score')
for i, v in enumerate(results_df['F1-Score']):
    axes[1].text(v + 0.01, i, f'{v:.4f}', va='center')

# ROC-AUC
axes[2].barh(results_df['Model'], results_df['ROC-AUC'], color='mediumseagreen')
axes[2].set_xlabel('ROC-AUC')
axes[2].set_title('Test ROC-AUC')
for i, v in enumerate(results_df['ROC-AUC']):
    axes[2].text(v + 0.005, i, f'{v:.4f}', va='center')

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… ì°¨íŠ¸ ì €ì¥: model_comparison.png")

# %% 8. ROC Curve
from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 8))

for name, config in models.items():
    if 'y_proba' in config:
        fpr, tpr, _ = roc_curve(y_test, config['y_proba'])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5000)')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve ë¹„êµ', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', fontsize=10)
plt.grid(alpha=0.3)
plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… ROC ì°¨íŠ¸ ì €ì¥: roc_curves.png")

# %% 9. Feature Importance (íŠ¸ë¦¬ ê¸°ë°˜)
for name, config in models.items():
    if name in ['RF', 'GB', 'XGB', 'LGB'] and 'trained' in config:
        model = config['trained']

        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            fi_df = pd.DataFrame({
                'Feature': X_train.columns,
                'Importance': importances
            }).sort_values('Importance', ascending=False).head(15)

            plt.figure(figsize=(10, 6))
            plt.barh(fi_df['Feature'], fi_df['Importance'], color='teal')
            plt.xlabel('Importance')
            plt.title(f'{name} - Feature Importance (Top 15)', fontweight='bold')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.savefig(f'fi_{name}.png', dpi=300, bbox_inches='tight')
            plt.show()

            print(f"âœ… {name} Feature Importance ì €ì¥")

# %% 10. ëª¨ë¸ ì €ì¥
import pickle

best_model_name = best['Model']
best_model = models[best_model_name]['trained']

with open(f'best_model_{best_model_name}.pkl', 'wb') as f:
    pickle.dump(best_model, f)

print(f"âœ… ìµœê³  ëª¨ë¸ ì €ì¥: best_model_{best_model_name}.pkl")

# ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥ (í•„ìš”ì‹œ)
if models[best_model_name]['scaled']:
    with open('scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: scaler.pkl")

# %% 11. Classification Report
print("\n" + "="*60)
print(f"ğŸ“Š {best_model_name} - Classification Report")
print("="*60)
best_y_pred = best_model.predict(X_test_scaled if models[best_model_name]['scaled'] else X_test)
print(classification_report(y_test, best_y_pred, target_names=['No Purchase', 'Purchase']))

print("\nâœ¨ ì™„ë£Œ!")
